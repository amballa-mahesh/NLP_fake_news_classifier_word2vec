# -*- coding: utf-8 -*-
"""NLP_fake_news_classifier_word2vec.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UicZXa1UXpWAg_tr26pqyZK_F3FXGLzk
"""

from google.colab import drive
drive.mount('/content/mydrive')

import pandas as pd

"""Reading Data Using Pandas"""

df = pd.read_csv('/content/mydrive/MyDrive/Fake News Data/train.csv')

df.head()

df.isnull().sum()

"""Removing Null Data"""

df.dropna(inplace =True,axis=0)
df.reset_index(inplace=True)

df.isnull().sum()

df.shape

df.head()

df['label'].value_counts()

import re
import string
from tqdm import tqdm
import nltk
from nltk.stem import WordNetLemmatizer
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')
from nltk.corpus import stopwords 
from nltk import word_tokenize,sent_tokenize
from nltk.stem import PorterStemmer

stops = stopwords.words('english')
lm = WordNetLemmatizer()
ps = PorterStemmer()

stops

"""Removing some words from the stop words

```
# This is formatted as code
```


"""

stops.remove('no')
stops.remove('not')
stops.remove('nor')

"""Creating X,Y"""

y = df['label']

x = df.drop(columns='label',axis =1)

"""# Cleaning the text data and building words corpus"""

from nltk.corpus.reader import wordlist
corpus = []
for i in tqdm(range(0,x.shape[0])):
  text = re.sub(r"didn't", "did not", df['text'][i])
  text = re.sub(r"don't", "do not", text)
  text = re.sub(r"won't", "will not", text)
  text = re.sub(r"can't", "can not", text)
  text = re.sub(r"wasn't", "do not", text)
  text = re.sub(r"should't", "should not", text)
  text = re.sub(r"could't", "could not", text)  
  text = re.sub(r"\'ve", " have", text)
  text = re.sub(r"\'m", " am", text)
  text = re.sub(r"\'ll", " will", text)
  text = re.sub(r"\'re", " are", text)
  text = re.sub(r"\'s", " is", text)
  text = re.sub(r"\'d", " would", text)
  text = re.sub(r"\'t", " not", text)
  text = re.sub(r"\'m", " am", text)
  text = re.sub(r"n\'t", " not", text)
  text = re.sub('[^a-zA-Z]',' ',text)
  text = text.lower()
  text = text.split()
  text = [ps.stem(word) for word in text if word not in stops]
  text = ' '.join(text)
  corpus.append(text)
word_corpus = []
for sent in tqdm(corpus):
  sent_token = sent_tokenize(sent)
  for sen in sent_token:
    words = word_tokenize(sen)
  word_corpus.append(words)

word_corpus[0:2]

len(word_corpus)

!pip install gensim

import gensim
print(gensim.__version__)

"""# Creating the Gensim Model - CBOW (sg=0)"""

model = gensim.models.Word2Vec(word_corpus, vector_size =100,min_count = 2, epochs=10,window=5,sg =0)

model.vector_size

model.wv.vectors.shape

model.wv.similar_by_word('good')

import numpy as np

"""# *Creating Average Word to Vec for all similar shaped vectors*"""

def avgword2vec(doc):
  return np.mean([model.wv[word] for word in doc if word in model.wv.index_to_key],axis =0)

"""**Creating Avg word 2 vec Vectors with size 100.**"""

X = []
for i in tqdm(range(len(word_corpus))):
  X.append(avgword2vec(word_corpus[i]))

y.shape

len(X)

"""We are using Minmaxscaler for creating all the postive value vectors as error popping while use using the raw negative values...."""

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

scaler.fit(X)

x_scaled = scaler.transform(X)

len(x_scaled)

x_scaled[0][0]

from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test = train_test_split(x_scaled,y,test_size= 0.2, stratify = y,random_state = 123)

len(x_train),len(x_test),len(y_train),len(y_test)

x_train[0]

# Commented out IPython magic to ensure Python compatibility.
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import GridSearchCV
import math
from sklearn.metrics import roc_curve,auc, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

"""Creating Model using MultinomialNB and GridSearchCV"""

l =[0.5,0.5]
alphas = np.array([0.0001,0.001,0.01,0.1,1,10,100,1000,10000])
param_grid = {'alpha':alphas}
estimator = MultinomialNB(class_prior = l,fit_prior = False)
neigh = GridSearchCV(estimator,param_grid,scoring='roc_auc',cv = 5,return_train_score=True,verbose=4)
neigh.fit(x_train,y_train)

results = pd.DataFrame.from_dict(neigh.cv_results_)

best_param = results['param_alpha']
train_auc  = results['mean_train_score']
test_auc   = results['mean_test_score']
log_alphas = []
for i in range(0,len(best_param)):
  value = math.log10(best_param[i])
  log_alphas.append(value)
log_alphas.sort()

neigh.best_params_['alpha']

"""Accuracy Graph by alphas values"""

plt.figure(figsize=(5,3))
plt.plot(log_alphas,train_auc,label = 'train_auc')
plt.plot(log_alphas,test_auc, label = 'test_auc')
plt.scatter(log_alphas, train_auc, label='Train AUC points')
plt.scatter(log_alphas, test_auc, label='Test AUC points')
plt.legend()
plt.show()
print(neigh.best_params_)
print(neigh.best_score_)
print('best alpha log value-  ',math.log10(neigh.best_params_['alpha']))

"""# Finding the Thershold value using the ROC- AUC"""

l = [0.5,0.5]
estimator = MultinomialNB(alpha=neigh.best_params_['alpha'],class_prior = l,fit_prior = False)
estimator.fit(x_train,y_train)

y_train_prob = estimator.predict_proba(x_train)[:,1]
y_test_prob  = estimator.predict_proba(x_test)[:,1]

train_fpr,train_tpr, train_thershold = roc_curve(y_train,y_train_prob)
test_fpr, test_tpr,  test_thershold  = roc_curve(y_test,y_test_prob)

plt.figure(figsize=(5,4))
plt.plot(train_fpr, train_tpr, label = "train_auc = "+str(auc(train_fpr,train_tpr)))
plt.plot(test_fpr,  test_tpr,  label = "test_auc = "+str(auc(test_fpr,  test_tpr)))
plt.legend()
plt.grid()
plt.show()

thershold = train_thershold[np.argmax(train_tpr*(1-train_fpr))]
print(" best Thershold is - ", round(thershold,2))
print('Maximum value of tpr*(1-fpr) is - ',round(max(train_tpr*(1-train_fpr)),2))

"""# Confusion Matrix evaluation - Without Thershold Value"""

y_train_pred = estimator.predict(x_train)
y_test_pred  = estimator.predict(x_test)
plt.figure(figsize= (3,2))
sns.heatmap(confusion_matrix(y_train, y_train_pred), annot=True,fmt='d')
plt.ylabel('actual values')
plt.xlabel('predicted values')
plt.show()
plt.figure(figsize= (3,2))
sns.heatmap(confusion_matrix(y_test, y_test_pred)  ,annot=True,fmt='d')
plt.ylabel('actual values')
plt.xlabel('predicted values')
plt.show()

"""# Confusion Matrix evaluation - Thershold Value"""

predictions_train = []
for i in y_train_prob:
  if i>= thershold:
    predictions_train.append(1)
  else:
    predictions_train.append(0)
    
predictions_test = []
for i in y_test_prob:
  if i>= thershold:
    predictions_test.append(1)
  else:
    predictions_test.append(0)

plt.figure(figsize= (3,2))
sns.heatmap(confusion_matrix(y_train, predictions_train), annot=True,fmt='d')
plt.ylabel('actual values')
plt.xlabel('predicted values')
plt.show()
plt.figure(figsize= (3,2))
sns.heatmap(confusion_matrix(y_test, predictions_test )  ,annot=True,fmt='d')
plt.ylabel('actual values')
plt.xlabel('predicted values')
plt.show()